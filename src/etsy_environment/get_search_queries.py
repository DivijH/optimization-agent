import os
import json
import argparse
from pathlib import Path
from typing import List, Any
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser

os.environ["OPENAI_API_KEY"] = open(Path(__file__).resolve().parents[1] / "keys" / "litellm.key").read().strip()
os.environ["OPENAI_API_BASE"] = "https://litellm.litellm.kn.ml-platform.etsy-mlinfra-dev.etsycloud.com"

SUBTASK_GENERATION_PROMPT = """
You are a semantic search assistant. Given a persona and a task, output a JSON array of concise search phrases that this person would realistically type into a search engine.

Guidelines:
- Output 1 to 5 search-friendly phrases as a JSON array.
- Keep each phrase short (ideally 2-5 words).
- Make each phrase natural and specific, not overly verbose.
- Reflect the user's persona (age, income, decision style, etc.) in how broad or specific the queries are.
- Do not include years, trends, or outdated references.
- Avoid overly repeated adjectives like "best," "top-rated," unless natural.
- Do not combine different search items into one phrase unless it is natural.
- Do not include verbs like "buy" or "add to cart" in the search phrases.
- Return only the JSON array and nothing else.
""".strip()


def generate_queries(persona: str, task: str, *, model_name: str = "gpt-4o-mini", temperature: float = 0.7) -> List[str]:
    """Generate search queries for a single persona + task using an LLM."""

    llm = ChatOpenAI(temperature=temperature, model=model_name)
    parser = JsonOutputParser()

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", SUBTASK_GENERATION_PROMPT),
            (
                "user",
                "PERSONA: {persona}\nTASK: {task}",
            ),
        ]
    )

    chain = prompt | llm | parser

    try:
        raw = chain.invoke({"persona": persona, "task": task})
    except Exception as e:
        raise RuntimeError(f"LLM error while generating queries: {e}")

    queries: List[str] = []
    if isinstance(raw, list):
        queries = [str(item) for item in raw]
    else:
        raise ValueError(f"Unexpected LLM output format: {raw}")

    if not queries:
        raise ValueError("No queries generated by LLM.")

    return queries


def process_persona_file(file_path: Path, model_name: str, temperature: float, overwrite: bool = False) -> None:
    """Generate queries for a single persona JSON file and write them back."""
    with file_path.open("r", encoding="utf-8") as f:
        data: Any = json.load(f)

    if not overwrite and "search_queries" in data and data["search_queries"]:
        # print(f"[SKIP] {file_path.name} already contains search_queries.")
        return

    persona_text: str = data.get("persona", "")
    task_text: str = data.get("intent", "")

    if not persona_text or not task_text:
        print(f"[WARN] {file_path.name} missing persona or intent field - skipping.")
        return

    # Process file silently on success
    try:
        queries = generate_queries(persona_text, task_text, model_name=model_name, temperature=temperature)
        data["search_queries"] = queries
        with file_path.open("w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"❌ {file_path.name} - {e}")


def main() -> None:
    parser = argparse.ArgumentParser(description="Generate search queries for all persona JSON files.")
    parser.add_argument("--personas-dir", default=Path(__file__).resolve().parents[2] / "data" / "personas", type=Path, help="Directory containing persona JSON files.")
    parser.add_argument("--overwrite", action="store_true", help="Overwrite existing search_queries if they are already present.")
    parser.add_argument("--model", default="gpt-4o-mini", help="OpenAI model name to use.")
    parser.add_argument("--temperature", type=float, default=0.7, help="Sampling temperature.")
    parser.add_argument("--workers", type=int, default=os.cpu_count() or 4, help="Number of parallel worker threads.")
    args = parser.parse_args()

    persona_dir: Path = args.personas_dir
    if not persona_dir.is_dir():
        parser.error(f"Persona directory not found: {persona_dir}")

    files = sorted(p for p in persona_dir.iterdir() if p.suffix == ".json")
    if not files:
        print("No persona JSON files found.")
        return

    def _worker(path: Path) -> None:
        """Wrapper to call process_persona_file with CLI arguments."""
        process_persona_file(
            path,
            model_name=args.model,
            temperature=args.temperature,
            overwrite=args.overwrite,
        )

    # Run workers in parallel with a progress bar
    with ThreadPoolExecutor(max_workers=max(1, args.workers)) as executor:
        list(tqdm(executor.map(_worker, files), total=len(files)))


if __name__ == "__main__":
    main()
